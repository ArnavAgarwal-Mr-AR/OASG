{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9799342,"sourceType":"datasetVersion","datasetId":6005432}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install accelerate peft bitsandbytes transformers trl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset, Dataset\nfrom peft import LoraConfig, AutoPeftModelForCausalLM\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, GenerationConfig\nfrom trl import SFTTrainer\nimport os\nimport random\nimport sys\nfrom time import perf_counter\nimport re","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"Llama3.1 key\")\nsecret_value_1 = user_secrets.get_secret(\"WandB key\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(secret_value_0)\nprint(\"Successfully logged in to Hugging Face!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_id=\"MrAR/NarrativeStoryGen\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_model_and_tokenizer(model_id):\n    tokenizer = AutoTokenizer.from_pretrained(model_id)\n    tokenizer.pad_token = tokenizer.eos_token\n    bnb_config = BitsAndBytesConfig(\n        load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n    )\n    model = AutoModelForCausalLM.from_pretrained(\n        model_id, quantization_config=bnb_config, device_map=\"auto\"\n    )\n    model.config.use_cache=False\n    model.config.pretraining_tp=1\n    return model, tokenizer","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, tokenizer = get_model_and_tokenizer(model_id)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system_message = \"\"\"You are an AI assistant specializing in creative writing, particularly short stories. Your task is to create an engaging, coherent, and imaginative narrative based on the prompts given. Please follow these guidelines:\n\n1. Focus on creating a continuous, flowing narrative. Each response should pick up exactly where the last one left off.\n2. Develop the story progressively, maintaining strict continuity with previous parts of the narrative.\n3. Include vivid descriptions and character development to make the story immersive.\n4. Keep your responses focused solely on the story. Do not include any meta-commentary or questions to the user.\n5. Aim for 3-4 paragraphs per response, ensuring each paragraph flows smoothly into the next.\n6. End each response at a natural breaking point in the story, ready to be continued in the next prompt.\n\nRemember, you are creating a single, continuous story across multiple prompts. Maintain perfect coherence and build upon previous elements without repetition.\"\"\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class ChatMemory:\n    def __init__(self, max_turns=30):\n        self.conversations = []\n        self.max_turns = max_turns\n\n    def add_interaction(self, role, content):\n        self.conversations.append({\"role\": role, \"content\": content})\n        if len(self.conversations) > self.max_turns * 2:\n            self.conversations = self.conversations[-self.max_turns * 2:]\n\n    def get_conversation_history(self):\n        return self.conversations\n\nchat_memory = ChatMemory()\n\ndef generate_response(user_input):\n    chat_memory.add_interaction(\"user\", user_input)\n    \n    prompt = formatted_prompt(user_input)\n    generation_config = GenerationConfig(\n        do_sample=True,\n        top_k=50,\n        top_p=0.95,\n        temperature=0.7,\n        repetition_penalty=1.1,\n        max_new_tokens=1000,\n        pad_token_id=tokenizer.eos_token_id,\n        eos_token_id=tokenizer.eos_token_id,\n    )\n    \n    start_time = perf_counter()\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to('cuda')\n    outputs = model.generate(**inputs, generation_config=generation_config)\n    \n    full_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n    new_response = full_response.split(\"<|im_start|>assistant\")[-1].strip()\n    \n    # Clean up the response\n    new_response = re.sub(r'<\\|im_end\\|>.*', '', new_response, flags=re.DOTALL)\n    new_response = new_response.replace('<|im_end|>', '').strip()\n    \n    # Remove any meta-commentary\n    new_response = re.sub(r'I hope this.*', '', new_response, flags=re.DOTALL)\n    new_response = re.sub(r'Let me know if.*', '', new_response, flags=re.DOTALL)\n    \n    chat_memory.add_interaction(\"assistant\", new_response)\n    \n    print(new_response)\n    output_time = perf_counter() - start_time\n    print(f\"Time taken for inference: {round(output_time,2)} seconds\")\n    \n    return new_response\n\n\ndef formatted_prompt(question) -> str:\n    conversation_history = chat_memory.get_conversation_history()\n    context = \"\\n\".join([f\"<|im_start|>{turn['role']}\\n{turn['content']}<|im_end|>\" for turn in conversation_history])\n    \n    return f\"<|im_start|>system\\n{system_message}<|im_end|>\\n{context}\\n<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant:\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ans = generate_response('Expand the fragment into a short story: \"A car accident involving a child not another vehicle. It was very personal and involved me. It was also a bit traumatizing.\"')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def get_human_scores(story, evaluator_id):\n    print(f\"Evaluator {evaluator_id}, please rate the following story:\")\n    print(story)\n    \n    fluency = int(input(\"Fluency (1-5): \"))\n    coherence = int(input(\"Coherence (1-5): \"))\n    creativity = int(input(\"Creativity (1-5): \"))\n    engagement = int(input(\"Engagement (1-5): \"))\n    \n    return {'fluency': fluency, 'coherence': coherence, 'creativity': creativity, 'engagement': engagement}\n\nstory = ans\nevaluator_scores = get_human_scores(story, 1)\nprint(f\"Evaluator Scores: {evaluator_scores}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ans=\"\"\"As I pulled up to the intersection, something caught my eye out of the corner of it. A young boy in his mother's arms, huddled together under a blanket while she tried frantically to flag down passing cars. At first, I thought they were just waiting for someone. But then I saw the crumpled metal of their compact sedan in the middle of the road, steam rising from its damaged hood. \n\nMy heart sank as I parked and ran over to them. The mother looked up at me, her face pale but determined. \"Please help us,\" she pleaded. \"We need to get him to a hospital.\"\n\nI knelt beside the small figure, who stared up at me unblinkingly. He seemed to be okay physically - no bleeding or broken bones that I could see. But there was something about his calm demeanor that sent a chill down my spine. As if he had seen something beyond what we could comprehend.\n\nTogether, we managed to load him into the backseat of my car. The mother climbed in after him, holding him close. And that's when I noticed the blood stains on her clothes, and realized that the accident must have been her fault. She had run a red light and collided head-on with an unseen obstacle.\n\nWe drove to the nearest hospital emergency room, the boy's still form in the back seat. As we walked inside, I couldn't shake the feeling that something wasn't right. That this incident was more than just a tragic accident. But I pushed those thoughts aside, focusing instead on getting the little boy the help he needed.\n\nThe doctors rushed him away for tests, leaving his mother and I to wait anxiously in the sterile waiting area. Time passed slowly, each minute stretching into an eternity. Finally, the doctor emerged, his expression grim. \"He has a severe concussion,\" he said gravely. \"But he'll pull through with proper care.\"\n\nRelief washed over me, and I turned to embrace the mother, who sobbed silently against my shoulder. In that moment, I realized how lucky we all were. How easily things could have gone so much worse. And I made a silent vow to myself: never again would I let a child suffer because of my negligence.\n\nThe accident was traumatic, both personally and emotionally. It changed the way I viewed life, reminding me of the fragility of existence and the importance of cherishing every moment. But it also taught me a lesson in compassion and empathy, one I carry with me always.\n\nThat day marked the beginning of a new chapter in my life, one filled with renewed purpose and a deeper understanding of the world around me. And though I know it will never fully leave me, I take comfort in knowing that I did everything I could to prevent further harm. To honor the memory of that brave little boy, whose resilience and strength continue to inspire me even now.\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:42:26.192968Z","iopub.execute_input":"2024-11-17T10:42:26.193314Z","iopub.status.idle":"2024-11-17T10:42:26.200443Z","shell.execute_reply.started":"2024-11-17T10:42:26.193280Z","shell.execute_reply":"2024-11-17T10:42:26.199251Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from nltk import word_tokenize\n\ndef type_token_ratio(text):\n    tokens = word_tokenize(text)\n    types = set(tokens)\n    return len(types) / len(tokens)\n\ntext = ans\nlexical_diversity = type_token_ratio(text)\nprint(f\"Lexical Diversity (Type-Token Ratio): {lexical_diversity}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:42:26.215060Z","iopub.execute_input":"2024-11-17T10:42:26.215644Z","iopub.status.idle":"2024-11-17T10:42:27.789118Z","shell.execute_reply.started":"2024-11-17T10:42:26.215611Z","shell.execute_reply":"2024-11-17T10:42:27.788155Z"}},"outputs":[{"name":"stdout","text":"Lexical Diversity (Type-Token Ratio): 0.5499124343257443\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nos.chdir(\"/kaggle/input/indictrans2/IndicTrans2-main/IndicTrans2-main/huggingface_interface\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:44:39.017847Z","iopub.execute_input":"2024-11-17T10:44:39.018770Z","iopub.status.idle":"2024-11-17T10:44:39.030424Z","shell.execute_reply.started":"2024-11-17T10:44:39.018725Z","shell.execute_reply":"2024-11-17T10:44:39.029542Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"!python3 -m pip install nltk sacremoses pandas regex mock transformers>=4.33.2 mosestokenizer\n!python3 -c \"import nltk; nltk.download('punkt')\"\n!python3 -m pip install bitsandbytes scipy accelerate datasets\n!python3 -m pip install sentencepiece","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"os.chdir(\"/kaggle/input/indictrans2/IndicTransToolkit-main/IndicTransToolkit-main/IndicTransToolkit/\")\n!python3 -m pip install --editable ./","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\n!pip install sacremoses\n!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:45:15.911044Z","iopub.execute_input":"2024-11-17T10:45:15.911387Z","iopub.status.idle":"2024-11-17T10:45:39.473166Z","shell.execute_reply.started":"2024-11-17T10:45:15.911351Z","shell.execute_reply":"2024-11-17T10:45:39.471916Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/indictrans2/IndicTransToolkit-main/IndicTransToolkit-main') \nsys.path.append('/kaggle/input/indictrans2/indic_nlp_library-master/indic_nlp_library-master')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:48:16.729941Z","iopub.execute_input":"2024-11-17T10:48:16.730315Z","iopub.status.idle":"2024-11-17T10:48:16.735262Z","shell.execute_reply.started":"2024-11-17T10:48:16.730279Z","shell.execute_reply":"2024-11-17T10:48:16.734248Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, BitsAndBytesConfig, AutoTokenizer\nfrom IndicTransToolkit import IndicProcessor\nBATCH_SIZE = 4\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nquantization = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:48:19.267209Z","iopub.execute_input":"2024-11-17T10:48:19.267613Z","iopub.status.idle":"2024-11-17T10:48:32.227415Z","shell.execute_reply.started":"2024-11-17T10:48:19.267573Z","shell.execute_reply":"2024-11-17T10:48:32.226578Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def initialize_model_and_tokenizer(ckpt_dir, quantization):\n    if quantization == \"4-bit\":\n        qconfig = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_compute_dtype=torch.bfloat16,\n        )\n    elif quantization == \"8-bit\":\n        qconfig = BitsAndBytesConfig(\n            load_in_8bit=True,\n            bnb_8bit_use_double_quant=True,\n            bnb_8bit_compute_dtype=torch.bfloat16,\n        )\n    else:\n        qconfig = None\n\n    tokenizer = AutoTokenizer.from_pretrained(ckpt_dir, trust_remote_code=True)\n    model = AutoModelForSeq2SeqLM.from_pretrained(\n        ckpt_dir,\n        trust_remote_code=True,\n        low_cpu_mem_usage=True,\n        quantization_config=qconfig,\n    )\n\n    if qconfig == None:\n        model = model.to(DEVICE)\n        if DEVICE == \"cuda\":\n            model.half()\n\n    model.eval()\n\n    return tokenizer, model\n\n\ndef batch_translate(input_sentences, src_lang, tgt_lang, model, tokenizer, ip):\n    translations = []\n    for i in range(0, len(input_sentences), BATCH_SIZE):\n        batch = input_sentences[i : i + BATCH_SIZE]\n\n        # Preprocess the batch and extract entity mappings\n        batch = ip.preprocess_batch(batch, src_lang=src_lang, tgt_lang=tgt_lang)\n\n        # Tokenize the batch and generate input encodings\n        inputs = tokenizer(\n            batch,\n            truncation=True,\n            padding=\"longest\",\n            return_tensors=\"pt\",\n            return_attention_mask=True,\n        ).to(DEVICE)\n\n        # Generate translations using the model\n        with torch.no_grad():\n            generated_tokens = model.generate(\n                **inputs,\n                use_cache=True,\n                min_length=0,\n                max_length=256,\n                num_beams=5,\n                num_return_sequences=1,\n            )\n\n        # Decode the generated tokens into text\n\n        with tokenizer.as_target_tokenizer():\n            generated_tokens = tokenizer.batch_decode(\n                generated_tokens.detach().cpu().tolist(),\n                skip_special_tokens=True,\n                clean_up_tokenization_spaces=True,\n            )\n\n        # Postprocess the translations, including entity replacement\n        translations += ip.postprocess_batch(generated_tokens, lang=tgt_lang)\n\n        del inputs\n        torch.cuda.empty_cache()\n\n    return translations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:48:32.228957Z","iopub.execute_input":"2024-11-17T10:48:32.229572Z","iopub.status.idle":"2024-11-17T10:48:32.241888Z","shell.execute_reply.started":"2024-11-17T10:48:32.229530Z","shell.execute_reply":"2024-11-17T10:48:32.240779Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def split_sentences(text):\n    '''\n    Given a text, return a list of sentences.\n    '''\n    ### Split on '. ', '.\\n', '.\\n\\n', '!', '?', and ';'\n    sentences = re.split(r'\\. |\\.\\n|\\.\\n\\n|!|\\?|;', text)\n    \n    ### Strip whitespace from each sentence\n    sentences = [\n        sentence.strip() + '..'\n        for sentence in sentences\n    ]\n    \n    ### Remove empty strings from the list of sentences\n    sentences = list(filter(None, sentences))\n    \n    number_of_sentences = len(sentences)\n    \n    return sentences[:-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:48:32.243288Z","iopub.execute_input":"2024-11-17T10:48:32.243671Z","iopub.status.idle":"2024-11-17T10:48:32.273269Z","shell.execute_reply.started":"2024-11-17T10:48:32.243625Z","shell.execute_reply":"2024-11-17T10:48:32.272470Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"import re","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:49:26.662400Z","iopub.execute_input":"2024-11-17T10:49:26.663367Z","iopub.status.idle":"2024-11-17T10:49:26.667699Z","shell.execute_reply.started":"2024-11-17T10:49:26.663314Z","shell.execute_reply":"2024-11-17T10:49:26.666736Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"en_indic_ckpt_dir = \"ai4bharat/indictrans2-en-indic-1B\"  # ai4bharat/indictrans2-en-indic-dist-200M\nen_indic_tokenizer, en_indic_model = initialize_model_and_tokenizer(en_indic_ckpt_dir, quantization)\n\nip = IndicProcessor(inference=True)\n\nen_sents = split_sentences(ans)\n\nsrc_lang, tgt_lang = \"eng_Latn\", \"hin_Deva\"\nhi_translations = batch_translate(en_sents, src_lang, tgt_lang, en_indic_model, en_indic_tokenizer, ip)\n\ntranslated_text = \"\"\nsource_text = \"\"\n\nfor input_sentence, translation in zip(en_sents, hi_translations):\n    source_text += f\"{input_sentence} \"  # Appends all source sentences into one\n    translated_text += f\"{translation} \"  # Appends all translations into one\n\nprint(f\"{src_lang}: {source_text.strip()}\")  # Prints the concatenated source text\nprint(f\"{tgt_lang}: {translated_text.strip()}\")  # Prints the concatenated translated text\n\n# flush the models to free the GPU memory\ndel en_indic_tokenizer, en_indic_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:49:28.742306Z","iopub.execute_input":"2024-11-17T10:49:28.742685Z","iopub.status.idle":"2024-11-17T10:49:39.116741Z","shell.execute_reply.started":"2024-11-17T10:49:28.742649Z","shell.execute_reply":"2024-11-17T10:49:39.115834Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4117: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"eng_Latn: As I pulled up to the intersection, something caught my eye out of the corner of it.. A young boy in his mother's arms, huddled together under a blanket while she tried frantically to flag down passing cars.. At first, I thought they were just waiting for someone.. But then I saw the crumpled metal of their compact sedan in the middle of the road, steam rising from its damaged hood.. My heart sank as I parked and ran over to them.. The mother looked up at me, her face pale but determined.. \"Please help us,\" she pleaded.. \"We need to get him to a hospital.\"\n\nI knelt beside the small figure, who stared up at me unblinkingly.. He seemed to be okay physically - no bleeding or broken bones that I could see.. But there was something about his calm demeanor that sent a chill down my spine.. As if he had seen something beyond what we could comprehend.. Together, we managed to load him into the backseat of my car.. The mother climbed in after him, holding him close.. And that's when I noticed the blood stains on her clothes, and realized that the accident must have been her fault.. She had run a red light and collided head-on with an unseen obstacle.. We drove to the nearest hospital emergency room, the boy's still form in the back seat.. As we walked inside, I couldn't shake the feeling that something wasn't right.. That this incident was more than just a tragic accident.. But I pushed those thoughts aside, focusing instead on getting the little boy the help he needed.. The doctors rushed him away for tests, leaving his mother and I to wait anxiously in the sterile waiting area.. Time passed slowly, each minute stretching into an eternity.. Finally, the doctor emerged, his expression grim.. \"He has a severe concussion,\" he said gravely.. \"But he'll pull through with proper care.\"\n\nRelief washed over me, and I turned to embrace the mother, who sobbed silently against my shoulder.. In that moment, I realized how lucky we all were.. How easily things could have gone so much worse.. And I made a silent vow to myself: never again would I let a child suffer because of my negligence.. The accident was traumatic, both personally and emotionally.. It changed the way I viewed life, reminding me of the fragility of existence and the importance of cherishing every moment.. But it also taught me a lesson in compassion and empathy, one I carry with me always.. That day marked the beginning of a new chapter in my life, one filled with renewed purpose and a deeper understanding of the world around me.. And though I know it will never fully leave me, I take comfort in knowing that I did everything I could to prevent further harm..\nhin_Deva: जैसे ही मैं चौराहे पर पहुँचा, उसके कोने से कुछ मेरी नज़र पकड़ लिया।  अपनी माँ की बाहों में एक युवा लड़का, एक कंबल के नीचे एक साथ इकट्ठा हुआ, जबकि उसने गुजरने वाली कारों को हरी झंडी दिखाने की कोशिश की।  पहले तो मुझे लगा कि वे किसी का इंतजार कर रहे हैं।  लेकिन फिर मैंने उनकी कॉम्पैक्ट सेडान की टूटी हुई धातु को सड़क के बीच में देखा, उसके क्षतिग्रस्त हुड से भाप उठ रही थी।  गाड़ी पार्क करते ही मेरा दिल टूट गया और मैं उनके पास भागा।  माँ ने मेरी ओर देखा, उनका चेहरा पीला लेकिन दृढ़ था।  \"कृपया हमारी मदद करें\", उसने विनती की।  \"हमें उसे अस्पताल ले जाना है।\" मैं घुटने टेककर उस छोटी सी मूर्ति के पास गया, जिसने मुझे बिना पलक झपकते देखा।  वह शारीरिक रूप से ठीक लग रहा था-कोई रक्तस्राव या टूटी हुई हड्डियाँ नहीं जो मैं देख सकता था।  लेकिन उनके शांत व्यवहार के बारे में कुछ ऐसा था जिसने मेरी रीढ़ को ठंडा कर दिया।  मानो उसने कुछ ऐसा देखा है जो हम समझ नहीं सकते थे।  हम सब मिलकर उसे अपनी कार की पिछली सीट पर बिठाने में कामयाब रहे।  माँ उसके पीछे चढ़ गई, उसे करीब से पकड़ लिया।  और तभी मैंने उसके कपड़ों पर खून के धब्बे देखे, और महसूस किया कि दुर्घटना उसकी गलती से हुई होगी।  उसने एक लाल बत्ती चलाई थी और एक अनदेखी बाधा से टकरा गई थी।  हम निकटतम अस्पताल के आपातकालीन कक्ष में चले गए, लड़का अभी भी पीछे की सीट पर फॉर्म में है।  जैसे ही हम अंदर गए, मैं इस भावना को हिला नहीं सका कि कुछ गलत था।  कि यह घटना सिर्फ एक दुखद दुर्घटना से अधिक थी..  लेकिन मैंने उन विचारों को एक तरफ धकेल दिया, इसके बजाय छोटे लड़के को वह मदद दिलाने पर ध्यान केंद्रित किया जिसकी उसे आवश्यकता थी।  डॉक्टरों ने उसे परीक्षण के लिए दूर ले जाया, उसकी माँ और मुझे बांझ प्रतीक्षा क्षेत्र में उत्सुकता से इंतजार करने के लिए छोड़ दिया।  समय धीरे-धीरे बीतता गया, हर मिनट अनंत काल तक फैलता गया।  अंत में, डॉक्टर बाहर आया, उसका भाव गंभीर था।  \"उसे गंभीर चोट लगी है\", उसने गंभीर रूप से कहा।  \"लेकिन वह उचित देखभाल के साथ आगे बढ़ेगा।\" राहत मुझ पर हावी हो गई, और मैं माँ को गले लगाने के लिए मुड़ गया, जो चुपचाप मेरे कंधे पर रो पड़ी।  उस पल में, मुझे एहसास हुआ कि हम सभी कितने भाग्यशाली थे।  कितनी आसानी से चीजें इतनी बदतर हो सकती थीं।  और मैंने अपने आप से एक मौन प्रतिज्ञा कीः मैं अपनी लापरवाही के कारण फिर कभी किसी बच्चे को पीड़ित नहीं होने दूंगा।  दुर्घटना व्यक्तिगत और भावनात्मक रूप से दर्दनाक थी।  इसने जीवन को देखने के मेरे तरीके को बदल दिया, मुझे अस्तित्व की नाजुकता और हर पल को संजोने के महत्व की याद दिला दी।  लेकिन इसने मुझे करुणा और सहानुभूति का एक सबक भी सिखाया, जिसे मैं हमेशा अपने साथ रखता हूं।  उस दिन मेरे जीवन में एक नए अध्याय की शुरुआत हुई, जो नए उद्देश्य और मेरे आसपास की दुनिया की गहरी समझ से भरा था।  और हालाँकि मुझे पता है कि यह मुझे कभी भी पूरी तरह से नहीं छोड़ेगा, मुझे यह जानकर आराम मिलता है कि मैंने आगे के नुकसान को रोकने के लिए हर संभव प्रयास किया।\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:49:52.115411Z","iopub.execute_input":"2024-11-17T10:49:52.115796Z","iopub.status.idle":"2024-11-17T10:49:52.120095Z","shell.execute_reply.started":"2024-11-17T10:49:52.115759Z","shell.execute_reply":"2024-11-17T10:49:52.119180Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"%%capture\n!wget https://github.com/AI4Bharat/Indic-TTS/releases/download/v1-checkpoints-release/hi.zip\n!unzip hi.zip\n!rm hi.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:49:52.947456Z","iopub.execute_input":"2024-11-17T10:49:52.947839Z","iopub.status.idle":"2024-11-17T10:50:19.823991Z","shell.execute_reply.started":"2024-11-17T10:49:52.947804Z","shell.execute_reply":"2024-11-17T10:50:19.822690Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"%%capture\n# 1. Create environment\n!sudo apt-get install libsndfile1-dev ffmpeg enchant\n!conda create -n tts-env -y\n!conda activate tts-env -y\n\n# 2. Setup PyTorch\n!pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n\n# 3. Setup Trainer\n!git clone https://github.com/gokulkarthik/Trainer ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:50:19.826091Z","iopub.execute_input":"2024-11-17T10:50:19.826451Z","iopub.status.idle":"2024-11-17T10:53:43.585732Z","shell.execute_reply.started":"2024-11-17T10:50:19.826412Z","shell.execute_reply":"2024-11-17T10:53:43.584092Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/Trainer\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:53:43.589434Z","iopub.execute_input":"2024-11-17T10:53:43.589889Z","iopub.status.idle":"2024-11-17T10:53:43.597021Z","shell.execute_reply.started":"2024-11-17T10:53:43.589837Z","shell.execute_reply":"2024-11-17T10:53:43.595053Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"%%capture\n!pip3 install -e .[all]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:53:43.600365Z","iopub.execute_input":"2024-11-17T10:53:43.600746Z","iopub.status.idle":"2024-11-17T10:54:19.603619Z","shell.execute_reply.started":"2024-11-17T10:53:43.600704Z","shell.execute_reply":"2024-11-17T10:54:19.602366Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:54:19.605180Z","iopub.execute_input":"2024-11-17T10:54:19.605532Z","iopub.status.idle":"2024-11-17T10:54:19.610461Z","shell.execute_reply.started":"2024-11-17T10:54:19.605494Z","shell.execute_reply":"2024-11-17T10:54:19.609589Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"%%capture\n# 4. Setup TTS\n!git clone https://github.com/gokulkarthik/TTS ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:54:19.611676Z","iopub.execute_input":"2024-11-17T10:54:19.612075Z","iopub.status.idle":"2024-11-17T10:54:25.741656Z","shell.execute_reply.started":"2024-11-17T10:54:19.612026Z","shell.execute_reply":"2024-11-17T10:54:25.740527Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"os.chdir(\"/kaggle/working/TTS\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:54:25.743174Z","iopub.execute_input":"2024-11-17T10:54:25.743520Z","iopub.status.idle":"2024-11-17T10:54:25.748311Z","shell.execute_reply.started":"2024-11-17T10:54:25.743485Z","shell.execute_reply":"2024-11-17T10:54:25.747247Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"%%capture\n!pip3 install -e .[all]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:54:25.749502Z","iopub.execute_input":"2024-11-17T10:54:25.749863Z","iopub.status.idle":"2024-11-17T10:57:39.642150Z","shell.execute_reply.started":"2024-11-17T10:54:25.749823Z","shell.execute_reply":"2024-11-17T10:57:39.640746Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"%%capture\nos.chdir(\"/kaggle/working/TTS\")\n\n# 5. Install other requirements\n! pip3 install -r requirements.txt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:57:39.643861Z","iopub.execute_input":"2024-11-17T10:57:39.644223Z","iopub.status.idle":"2024-11-17T10:57:52.329744Z","shell.execute_reply.started":"2024-11-17T10:57:39.644186Z","shell.execute_reply":"2024-11-17T10:57:52.328447Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"sys.path.append(\"/kaggle/working/TTS\")\nsys.path.append(\"/kaggle/working/hi\")\nsys.path.append(\"/kaggle/working/Trainer\")\nsys.path.append(\"/kaggle/working/TTS/TTS\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:57:52.334271Z","iopub.execute_input":"2024-11-17T10:57:52.334622Z","iopub.status.idle":"2024-11-17T10:57:52.339973Z","shell.execute_reply.started":"2024-11-17T10:57:52.334586Z","shell.execute_reply":"2024-11-17T10:57:52.338784Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"!sudo ln -sf /usr/local/cuda/lib64/libcusparse.so /usr/local/cuda/lib64/libcusparse.so.12\n!sudo ln -sf /usr/local/cuda/lib64/libnvJitLink.so /usr/local/cuda/lib64/libnvJitLink.so.12","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:57:52.341273Z","iopub.execute_input":"2024-11-17T10:57:52.341651Z","iopub.status.idle":"2024-11-17T10:57:54.452977Z","shell.execute_reply.started":"2024-11-17T10:57:52.341606Z","shell.execute_reply":"2024-11-17T10:57:54.451705Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"!mkdir -p /kaggle/working/TTS/models/v1/hi/fastpitch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:57:54.454496Z","iopub.execute_input":"2024-11-17T10:57:54.454825Z","iopub.status.idle":"2024-11-17T10:57:55.467236Z","shell.execute_reply.started":"2024-11-17T10:57:54.454789Z","shell.execute_reply":"2024-11-17T10:57:55.465860Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"!ln -s /kaggle/working/hi/fastpitch/speakers.pth /kaggle/working/TTS/models/v1/hi/fastpitch/speakers.pth","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:57:55.468782Z","iopub.execute_input":"2024-11-17T10:57:55.469113Z","iopub.status.idle":"2024-11-17T10:57:56.480218Z","shell.execute_reply.started":"2024-11-17T10:57:55.469066Z","shell.execute_reply":"2024-11-17T10:57:56.479088Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"!python3 -m TTS.bin.synthesize --text translated_text \\\n    --model_path /kaggle/working/hi/fastpitch/best_model.pth \\\n    --config_path /kaggle/working/hi/fastpitch/config.json \\\n    --speakers_file_path /kaggle/working/hi/fastpitch/speakers.pth\\\n    --vocoder_path /kaggle/working/hi/hifigan/best_model.pth \\\n    --vocoder_config_path /kaggle/working/hi/hifigan/config.json \\\n    --speaker_idx \"male\" \\\n    --out_path \"/kaggle/working/ouput.wav\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:59:17.471583Z","iopub.execute_input":"2024-11-17T10:59:17.472009Z","iopub.status.idle":"2024-11-17T10:59:31.457633Z","shell.execute_reply.started":"2024-11-17T10:59:17.471972Z","shell.execute_reply":"2024-11-17T10:59:31.456551Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"RuntimeError: module was compiled against NumPy C-API version 0x10 (NumPy 1.23) but the running NumPy has C-API version 0xe. Check the section C-API incompatibility at the Troubleshooting ImportError section at https://numpy.org/devdocs/user/troubleshooting-importerror.html#c-api-incompatibility for indications on how to solve this problem.\n > Using model: fast_pitch\n > Setting up Audio Processor...\n | > sample_rate:22050\n | > resample:False\n | > num_mels:80\n | > log_func:np.log\n | > min_level_db:-100\n | > frame_shift_ms:None\n | > frame_length_ms:None\n | > ref_level_db:20\n | > fft_size:1024\n | > power:1.5\n | > preemphasis:0.0\n | > griffin_lim_iters:60\n | > signal_norm:False\n | > symmetric_norm:True\n | > mel_fmin:0\n | > mel_fmax:8000.0\n | > pitch_fmin:0.0\n | > pitch_fmax:640.0\n | > spec_gain:1.0\n | > stft_pad_mode:reflect\n | > max_norm:4.0\n | > clip_norm:True\n | > do_trim_silence:True\n | > trim_db:60\n | > do_sound_norm:False\n | > do_amp_to_db_linear:True\n | > do_amp_to_db_mel:True\n | > do_rms_norm:False\n | > db_level:None\n | > stats_path:None\n | > base:2.718281828459045\n | > hop_length:256\n | > win_length:1024\n/kaggle/working/TTS/TTS/tts/utils/managers.py:20: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(f, map_location=\"cpu\")\n > Init speaker_embedding layer.\n/kaggle/working/TTS/TTS/tts/models/forward_tts.py:838: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  state = torch.load(checkpoint_path, map_location=torch.device(\"cpu\"))\n > Vocoder Model: hifigan\n > Setting up Audio Processor...\n | > sample_rate:22050\n | > resample:False\n | > num_mels:80\n | > log_func:np.log\n | > min_level_db:-100\n | > frame_shift_ms:None\n | > frame_length_ms:None\n | > ref_level_db:20\n | > fft_size:1024\n | > power:1.5\n | > preemphasis:0.0\n | > griffin_lim_iters:60\n | > signal_norm:False\n | > symmetric_norm:True\n | > mel_fmin:0\n | > mel_fmax:8000.0\n | > pitch_fmin:0.0\n | > pitch_fmax:640.0\n | > spec_gain:1.0\n | > stft_pad_mode:reflect\n | > max_norm:4.0\n | > clip_norm:True\n | > do_trim_silence:True\n | > trim_db:60\n | > do_sound_norm:False\n | > do_amp_to_db_linear:True\n | > do_amp_to_db_mel:True\n | > do_rms_norm:False\n | > db_level:None\n | > stats_path:None\n | > base:2.718281828459045\n | > hop_length:256\n | > win_length:1024\n > Generator Model: hifigan_generator\n/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n  WeightNorm.apply(module, name, dim)\n > Discriminator Model: hifigan_discriminator\n/kaggle/working/TTS/TTS/utils/io.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  return torch.load(f, map_location=map_location, **kwargs)\nRemoving weight norm...\n > Text: translated_text\n > Text splitted to sentences.\n['translated_text']\ntranslated_text\n [!] Character 't' not found in the vocabulary. Discarding it.\ntranslated_text\n [!] Character 'r' not found in the vocabulary. Discarding it.\ntranslated_text\n [!] Character 'a' not found in the vocabulary. Discarding it.\ntranslated_text\n [!] Character 'n' not found in the vocabulary. Discarding it.\ntranslated_text\n [!] Character 's' not found in the vocabulary. Discarding it.\ntranslated_text\n [!] Character 'l' not found in the vocabulary. Discarding it.\ntranslated_text\n [!] Character 'e' not found in the vocabulary. Discarding it.\ntranslated_text\n [!] Character 'd' not found in the vocabulary. Discarding it.\ntranslated_text\n [!] Character '_' not found in the vocabulary. Discarding it.\ntranslated_text\n [!] Character 'x' not found in the vocabulary. Discarding it.\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"/opt/conda/lib/python3.10/runpy.py\", line 86, in _run_code\n    exec(code, run_globals)\n  File \"/kaggle/working/TTS/TTS/bin/synthesize.py\", line 425, in <module>\n    main()\n  File \"/kaggle/working/TTS/TTS/bin/synthesize.py\", line 408, in main\n    wav = synthesizer.tts(\n  File \"/kaggle/working/TTS/TTS/utils/synthesizer.py\", line 301, in tts\n    outputs = synthesis(\n  File \"/kaggle/working/TTS/TTS/tts/utils/synthesis.py\", line 207, in synthesis\n    outputs = run_model_torch(\n  File \"/kaggle/working/TTS/TTS/tts/utils/synthesis.py\", line 50, in run_model_torch\n    outputs = _func(\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n    return func(*args, **kwargs)\n  File \"/kaggle/working/TTS/TTS/tts/models/forward_tts.py\", line 677, in inference\n    o_en, x_mask, g, _ = self._forward_encoder(x, x_mask, g)\n  File \"/kaggle/working/TTS/TTS/tts/models/forward_tts.py\", line 415, in _forward_encoder\n    o_en = self.encoder(torch.transpose(x_emb, 1, -1), x_mask) # [C, T] for single input; [B, C, T]\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/kaggle/working/TTS/TTS/tts/layers/feed_forward/encoder.py\", line 161, in forward\n    o = self.encoder(x, x_mask)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/kaggle/working/TTS/TTS/tts/layers/generic/transformer.py\", line 66, in forward\n    x, align = layer(x, src_key_padding_mask=mask)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/kaggle/working/TTS/TTS/tts/layers/generic/transformer.py\", line 29, in forward\n    src2 = self.conv2(F.relu(self.conv1(src)))\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 375, in forward\n    return self._conv_forward(input, self.weight, self.bias)\n  File \"/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 370, in _conv_forward\n    return F.conv1d(\nRuntimeError: Calculated padded input size per channel: (2). Kernel size: (3). Kernel size can't be greater than actual input size\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"!pip install librosa numpy matplotlib praat-parselmouth","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import librosa\nimport librosa.display\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport parselmouth\n\n# Load the audio file and detect the sampling rate\ndef load_audio(audio_path):\n    audio, sr = librosa.load(audio_path, sr=None)  # `sr=None` ensures original sampling rate is used\n    return audio, sr\n\n# Plot waveform\ndef plot_waveform(audio, sr, title=\"Waveform\"):\n    plt.figure(figsize=(10, 4))\n    librosa.display.waveshow(audio, sr=sr)\n    plt.title(title)\n    plt.xlabel(\"Time (s)\")\n    plt.ylabel(\"Amplitude\")\n    plt.show()\n\n# Extract F0 (pitch) using Parselmouth (Praat)\ndef extract_pitch(audio_path):\n    snd = parselmouth.Sound(audio_path)\n    pitch = snd.to_pitch()\n    pitch_values = pitch.selected_array['frequency']\n    pitch_values[pitch_values == 0] = np.nan  # Ignore unvoiced parts\n    return pitch_values\n\n# Plot pitch contour\ndef plot_pitch(audio_path, title=\"Pitch Contour\"):\n    pitch_values = extract_pitch(audio_path)\n    plt.figure(figsize=(10, 4))\n    plt.plot(pitch_values, label=\"F0 (Pitch)\")\n    plt.title(title)\n    plt.xlabel(\"Frame\")\n    plt.ylabel(\"Frequency (Hz)\")\n    plt.legend()\n    plt.show()\n\n# Plot mel spectrogram\ndef plot_mel_spectrogram(audio, sr, n_mels=80, hop_length=512, title=\"Mel Spectrogram\"):\n    mel_spec = librosa.feature.melspectrogram(audio, sr=sr, n_mels=n_mels, hop_length=hop_length)\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n\n    plt.figure(figsize=(10, 4))\n    librosa.display.specshow(mel_spec_db, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(title)\n    plt.xlabel(\"Time (s)\")\n    plt.ylabel(\"Mel Frequency\")\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"audio_path = \"/kaggle/working/ouput.wav\"  \naudio, sr = load_audio(audio_path)\nprint(f\"Detected Sampling Rate: {sr} Hz\")\n\nplot_waveform(audio_ta_female, sr, \"Generated Audio Waveform\")\nplot_pitch(audio_path, \"Generated Audio Pitch Contour\")\nplot_mel_spectrogram(audio_ta_female, sr, title=\"Generated Audio Mel Spectrogram\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:58:31.439120Z","iopub.status.idle":"2024-11-17T10:58:31.439481Z","shell.execute_reply.started":"2024-11-17T10:58:31.439306Z","shell.execute_reply":"2024-11-17T10:58:31.439329Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot mel spectrogram\ndef plot_mel_spectrogram(audio, sr, n_mels=80, hop_length=512, title=\"Mel Spectrogram\"):\n    mel_spec = librosa.feature.melspectrogram(audio, sr=sr, n_mels=n_mels, hop_length=hop_length)\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n\n    plt.figure(figsize=(10, 4))\n    librosa.display.specshow(mel_spec_db, sr=sr, hop_length=hop_length, x_axis='time', y_axis='mel')\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(title)\n    plt.xlabel(\"Time (s)\")\n    plt.ylabel(\"Mel Frequency\")\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:58:31.441144Z","iopub.status.idle":"2024-11-17T10:58:31.441514Z","shell.execute_reply.started":"2024-11-17T10:58:31.441338Z","shell.execute_reply":"2024-11-17T10:58:31.441357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_mel_spectrogram(audio, sr, title=\"Generated Audio Mel Spectrogram\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-17T10:58:31.442685Z","iopub.status.idle":"2024-11-17T10:58:31.443042Z","shell.execute_reply.started":"2024-11-17T10:58:31.442859Z","shell.execute_reply":"2024-11-17T10:58:31.442877Z"}},"outputs":[],"execution_count":null}]}